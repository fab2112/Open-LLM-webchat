# Open-LLM-webchat - Settings

# Users authentication
USERS = [
    ("user_1", "1234@1234"),
    ("user_2", "1234@1234"),
]

# Available LLM models
MODELS = (
    "Groq_openai/gpt-oss-120b",
    "Groq_openai/gpt-oss-20b",
    "Groq_meta-llama/llama-4-scout-17b-16e-instruct",
    "Groq_moonshotai/kimi-k2-instruct",
    "Groq_qwen/qwen3-32b",
    "Groq_meta-llama/llama-4-maverick-17b-128e-instruct",
    "Groq_deepseek-r1-distill-llama-70b",
    "Groq_llama-3.3-70b-versatile",
    "Google_gemini-2.5-pro",
    "Google_gemini-2.5-flash",
    "Google_gemini-2.5-flash-lite",
    "Google_gemini-2.0-flash",
    "Google_gemini-2.0-flash-lite",
    "OpenAI_gpt-5",
    "OpenAI_gpt-5-mini",
    "OpenAI_gpt-4o",
    "OpenAI_gpt-4o-mini",
    "Nvidia_qwen/qwen3-next-80b-a3b-instruct",
    "Nvidia_moonshotai/kimi-k2-instruct-0905",
    "Ollama_gemma3:1b",
)

# Data base path
DB_FILE = "tmp/agent.db"

# Chat stream time control
STREAM_DELAY = 0.01

# Ollama internal Docker url - binded to host by extra_hosts
OLLAMA_URL = "http://localhost:11434"

# Set numer of session runs for short-term memory
SHORT_MEM_RUNS = 3

# Set agent to verbose mode
DEBUG_MODE = True

# Maximum tokens generated by the model
MAX_TOKENS = 8000

